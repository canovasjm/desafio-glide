{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccc9a936",
   "metadata": {},
   "source": [
    "# Glide Technical Exercise  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4587ef",
   "metadata": {},
   "source": [
    "## 1) PySpark set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4335f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required classes\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e5dee51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/canovasjm/spark/spark-3.0.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/03/08 05:03:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# set up configuration\n",
    "conf = SparkConf() \\\n",
    "    .setMaster(\"local[*]\") \\\n",
    "    .setAppName(\"glide\") \\\n",
    "    .set(\"spark.jars\", \"/home/canovasjm/spark/spark-3.0.3-bin-hadoop3.2/jars/gcs-connector-hadoop3-latest.jar\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.json.keyfile\", \"/home/canovasjm/.google/credentials/google_credentials.json\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feac02e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bc92e0",
   "metadata": {},
   "source": [
    "## 2) Import CSV files to pyspark data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1affe7b2",
   "metadata": {},
   "source": [
    "### 2.1) Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0108219a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# read CSV files located in GCS bucket\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"gs://dtc_data_lake_deng-338919/glide/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926255d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first 5 rows to check if df looks fine\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba943fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the shape of df\n",
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a79aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the df schema. As we see, all columns are read as string\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac54c91",
   "metadata": {},
   "source": [
    "So far our all columns in our data frame are of type string. This is not optimal and won't allow us to do further manipulations of this data frame, so we need to fix it before moving on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c73c3d8",
   "metadata": {},
   "source": [
    "### 2.2) Read files with a proper schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9f1c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ee0e1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a proper schema\n",
    "schema = types.StructType([\n",
    "    types.StructField('snapshot_date', types.DateType(), True),\n",
    "    types.StructField('employee_number', types.IntegerType(), True),\n",
    "    types.StructField('status', types.StringType(), True),\n",
    "    types.StructField('first_name', types.StringType(), True),\n",
    "    types.StructField('last_name', types.StringType(), True),\n",
    "    types.StructField('gender', types.StringType(), True),\n",
    "    types.StructField('email', types.StringType(), True),\n",
    "    types.StructField('phone_number', types.StringType(), True),\n",
    "    types.StructField('salary', types.IntegerType(), True),\n",
    "    types.StructField('termination_date', types.DateType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eac56916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read CSV files located in GCS bucket again, this time specifying the schema we defined above\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"gs://dtc_data_lake_deng-338919/glide/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce9aa686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(snapshot_date,DateType,true),StructField(employee_number,IntegerType,true),StructField(status,StringType,true),StructField(first_name,StringType,true),StructField(last_name,StringType,true),StructField(gender,StringType,true),StructField(email,StringType,true),StructField(phone_number,StringType,true),StructField(salary,IntegerType,true),StructField(termination_date,DateType,true)))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the schema\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61888b34",
   "metadata": {},
   "source": [
    "## 3) Some data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3c7107a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Is there any record with employee_number NULL ?\n",
    "df.filter(\"employee_number IS NULL\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a7d2dde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+--------+----------+---------+------+------------------+------------+------+----------------+\n",
      "|snapshot_date|employee_number|  status|first_name|last_name|gender|             email|phone_number|salary|termination_date|\n",
      "+-------------+---------------+--------+----------+---------+------+------------------+------------+------+----------------+\n",
      "|   2020-01-04|             25|Inactive|    Stella|     Hunt|Female|s.hunt@newmail.com| 194-7397-62|122746|      2020-01-04|\n",
      "+-------------+---------------+--------+----------+---------+------+------------------+------------+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How many employees have a termination date different than NULL ?\n",
    "df.filter(\"termination_date IS NOT NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2484dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5658996a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 74:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+--------+----------+---------+------+------------------+------------+------+----------------+\n",
      "|snapshot_date|employee_number|  status|first_name|last_name|gender|             email|phone_number|salary|termination_date|\n",
      "+-------------+---------------+--------+----------+---------+------+------------------+------------+------+----------------+\n",
      "|   2020-01-01|             25|  Active|    Stella|     Hunt|Female|s.hunt@newmail.com| 194-7397-62|122746|            null|\n",
      "|   2020-01-02|             25|  Active|    Stella|     Hunt|Female|s.hunt@newmail.com| 194-7397-62|122746|            null|\n",
      "|   2020-01-03|             25|  Active|    Stella|     Hunt|Female|s.hunt@newmail.com| 194-7397-62|122746|            null|\n",
      "|   2020-01-04|             25|Inactive|    Stella|     Hunt|Female|s.hunt@newmail.com| 194-7397-62|122746|      2020-01-04|\n",
      "+-------------+---------------+--------+----------+---------+------+------------------+------------+------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Examine records for employee_number == 25\n",
    "df \\\n",
    "    .filter(\"employee_number == 25\") \\\n",
    "    .orderBy(F.col(\"snapshot_date\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b78cdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+------+----------+---------+------+--------------------+------------+------+----------------+\n",
      "|snapshot_date|employee_number|status|first_name|last_name|gender|               email|phone_number|salary|termination_date|\n",
      "+-------------+---------------+------+----------+---------+------+--------------------+------------+------+----------------+\n",
      "|   2020-01-01|              3|Active|    Sydney|  Farrell|Female|s.farrell@newmail...| 187-8343-84|151217|            null|\n",
      "|   2020-01-02|              3|Active|    Sydney|  Farrell|Female|s.farrell@newmail...| 187-8343-84|151217|            null|\n",
      "|   2020-01-03|              3|Active|    Sydney|  Farrell|Female|s.farrell@newmail...| 187-8343-84|151217|            null|\n",
      "|   2020-01-04|              3|Active|    Sydney|  Farrell|Female|s.farrell@newmail...| 187-8343-84|151217|            null|\n",
      "|   2020-01-05|              3|Active|    Sydney|  Farrell|Female|s.farrell@newmail...| 187-8343-84|151217|            null|\n",
      "|   2020-01-06|              3|Active|    Sydney|  Farrell|Female|s.farrell@newmail...| 187-8343-84|151217|            null|\n",
      "|   2020-01-07|              3|Active|    Sydney|  Farrell|Female|s.farrell@newmail...| 187-8343-84|151217|            null|\n",
      "|   2020-01-08|              3|Active|    Sydney|  Farrell|Female|s.farrell@newmail...| 187-8343-84|151217|            null|\n",
      "|   2020-01-09|              3|Active|    Sydney|  Farrell|Female|s.farrell@newmail...| 187-8343-84|151217|            null|\n",
      "|   2020-01-10|              3|Active|    Sydney|  Farrell|Female|s.farrell@newmail...| 187-8343-84|151217|            null|\n",
      "+-------------+---------------+------+----------+---------+------+--------------------+------------+------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Examine records for employee_number == 3 , which is duplicated on January 5th\n",
    "df \\\n",
    "    .filter(\"employee_number == 3\") \\\n",
    "    .orderBy(F.col(\"snapshot_date\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81c4626",
   "metadata": {},
   "source": [
    "## 4) Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "78ae81d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicated records\n",
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "afa068cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 79:=========================================>            (152 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(494, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 79:=================================================>    (184 + 4) / 200]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# print the shape of df, after romoving the duplicate\n",
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08156aff",
   "metadata": {},
   "source": [
    "## 5) Staging table: `employee_snapshot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "141dbdb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# save the df as staging table\n",
    "df.coalesce(1).write.csv(path='../data/processed/employee_snapshot/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd44055",
   "metadata": {},
   "source": [
    "## 6) Date aggregation: `valid_from` and `valid_to`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f092123a",
   "metadata": {},
   "source": [
    "In this step, my goal is to create a data frame containing `employee_number` and `status`, as well as two new columns with aggregated dates: `valid_from` and `valid_to`. \n",
    "\n",
    "My plan is to have, for each combination of employee and status, the date when the employee started and the date when the employee left the company. If the employee hasn't left the company yet, then I want to display the most recent snapshot date available.\n",
    "\n",
    "In the next step, I will join the data frame created here to the data frame I read in step 2.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79914874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a temp table from the data frame\n",
    "df.registerTempTable('table_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3dbef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the new columns with Spark SQL\n",
    "df_agg_dates = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    employee_number\n",
    "    , status\n",
    "    , MIN(snapshot_date) AS valid_from\n",
    "    , MAX(snapshot_date) AS valid_to\n",
    "FROM table_df\n",
    "GROUP BY employee_number, status\n",
    "ORDER BY employee_number, status\n",
    ";\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "92c2ff83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 84:==============================================>       (172 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+----------+----------+\n",
      "|employee_number|status|valid_from|  valid_to|\n",
      "+---------------+------+----------+----------+\n",
      "|              1|Active|2020-01-01|2020-01-10|\n",
      "|              2|Active|2020-01-01|2020-01-10|\n",
      "|              3|Active|2020-01-01|2020-01-10|\n",
      "|              4|Active|2020-01-01|2020-01-10|\n",
      "|              5|Active|2020-01-01|2020-01-10|\n",
      "|              6|Active|2020-01-01|2020-01-10|\n",
      "|              7|Active|2020-01-01|2020-01-10|\n",
      "|              8|Active|2020-01-01|2020-01-10|\n",
      "|              9|Active|2020-01-01|2020-01-10|\n",
      "|             10|Active|2020-01-01|2020-01-10|\n",
      "+---------------+------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# print some records of df_agg_dates to check how it looks\n",
    "df_agg_dates.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2994bd77",
   "metadata": {},
   "source": [
    "## 7) Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad68e53",
   "metadata": {},
   "source": [
    "### 7.1) Join previously created to data frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198d3f44",
   "metadata": {},
   "source": [
    "Now I will left join **df** and **df_agg_dates** on the composite key `['employee_number', 'status']` and save this to another data freme, named **df_result**. This way, **df_result** will have not only the original columns provided but also `valid_from` and `valid_to` created in 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1fc9bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the join\n",
    "df_result = df.join(df_agg_dates, on=['employee_number', 'status'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f849f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+-------------+----------+---------+------+----------------+----------+----------+\n",
      "|employee_number|status|snapshot_date|first_name|last_name|gender|termination_date|valid_from|  valid_to|\n",
      "+---------------+------+-------------+----------+---------+------+----------------+----------+----------+\n",
      "|              1|Active|   2020-01-01| Frederick|   Barnes|  Male|            null|2020-01-01|2020-01-10|\n",
      "|              1|Active|   2020-01-02| Frederick|   Barnes|  Male|            null|2020-01-01|2020-01-10|\n",
      "|              1|Active|   2020-01-03| Frederick|   Barnes|  Male|            null|2020-01-01|2020-01-10|\n",
      "|              1|Active|   2020-01-04| Frederick|   Barnes|  Male|            null|2020-01-01|2020-01-10|\n",
      "|              1|Active|   2020-01-05| Frederick|   Barnes|  Male|            null|2020-01-01|2020-01-10|\n",
      "|              1|Active|   2020-01-06| Frederick|   Barnes|  Male|            null|2020-01-01|2020-01-10|\n",
      "|              1|Active|   2020-01-07| Frederick|   Barnes|  Male|            null|2020-01-01|2020-01-10|\n",
      "|              1|Active|   2020-01-08| Frederick|   Barnes|  Male|            null|2020-01-01|2020-01-10|\n",
      "|              1|Active|   2020-01-09| Frederick|   Barnes|  Male|            null|2020-01-01|2020-01-10|\n",
      "|              1|Active|   2020-01-10| Frederick|   Barnes|  Male|            null|2020-01-01|2020-01-10|\n",
      "|              2|Active|   2020-01-01|    Alford|    Grant|  Male|            null|2020-01-01|2020-01-10|\n",
      "|              2|Active|   2020-01-02|    Alford|    Grant|  Male|            null|2020-01-01|2020-01-10|\n",
      "|              2|Active|   2020-01-03|    Alford|    Grant|  Male|            null|2020-01-01|2020-01-10|\n",
      "|              2|Active|   2020-01-04|    Alford|    Grant|  Male|            null|2020-01-01|2020-01-10|\n",
      "|              2|Active|   2020-01-05|    Alford|    Grant|  Male|            null|2020-01-01|2020-01-10|\n",
      "|              2|Active|   2020-01-06|    Alford|    Grant|  Male|            null|2020-01-01|2020-01-10|\n",
      "|              2|Active|   2020-01-07|    Alford|    Grant|  Male|            null|2020-01-01|2020-01-10|\n",
      "|              2|Active|   2020-01-08|    Alford|    Grant|  Male|            null|2020-01-01|2020-01-10|\n",
      "|              2|Active|   2020-01-09|    Alford|    Grant|  Male|            null|2020-01-01|2020-01-10|\n",
      "|              2|Active|   2020-01-10|    Alford|    Grant|  Male|            null|2020-01-01|2020-01-10|\n",
      "+---------------+------+-------------+----------+---------+------+----------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# take a look at df_result selecting only some columns\n",
    "df_result \\\n",
    "    .select(\"employee_number\",\"status\", \"snapshot_date\", \"first_name\", \"last_name\", \"gender\", \"termination_date\", \"valid_from\", \"valid_to\") \\\n",
    "    .orderBy(F.col(\"employee_number\"), F.col(\"snapshot_date\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6b23bb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a temp table from df_result to use Spark SQL\n",
    "df_result.registerTempTable('table_df_result')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a123cb6",
   "metadata": {},
   "source": [
    "### 7.2) Number rows ordered by `snapshot_date` descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "010a2701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b7d64825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define window specification\n",
    "windowSpec = Window \\\n",
    "    .partitionBy(\"employee_number\", \"status\") \\\n",
    "    .orderBy(F.col(\"snapshot_date\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2a5259de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number the rows in df_result partitioned by \"employee_number\" and \"status\" order by \"snapshot_date\" descending\n",
    "df_final = df_result \\\n",
    "    .withColumn(\"rn\",row_number().over(windowSpec)) \\\n",
    "    .filter(\"rn == 1\") \\\n",
    "    .orderBy(\"employee_number\", \"status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "be153aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 96:==========================================>           (159 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+-------------+----------+----------+------+--------------------+------------+------+----------------+----------+----------+---+\n",
      "|employee_number|  status|snapshot_date|first_name| last_name|gender|               email|phone_number|salary|termination_date|valid_from|  valid_to| rn|\n",
      "+---------------+--------+-------------+----------+----------+------+--------------------+------------+------+----------------+----------+----------+---+\n",
      "|              1|  Active|   2020-01-10| Frederick|    Barnes|  Male|f.barnes@newmail.com| 094-8926-78| 38582|            null|2020-01-01|2020-01-10|  1|\n",
      "|              2|  Active|   2020-01-10|    Alford|     Grant|  Male| a.grant@newmail.com| 389-8947-85| 53126|            null|2020-01-01|2020-01-10|  1|\n",
      "|              3|  Active|   2020-01-10|    Sydney|   Farrell|Female|s.farrell@newmail...| 187-8343-84|151217|            null|2020-01-01|2020-01-10|  1|\n",
      "|              4|  Active|   2020-01-10|     Rosie|  Richards|Female|r.richards@newmai...| 357-9337-53|162461|            null|2020-01-01|2020-01-10|  1|\n",
      "|              5|  Active|   2020-01-10|     Jared|    Wright|  Male|j.wright@newmail.com| 256-0454-59| 98258|            null|2020-01-01|2020-01-10|  1|\n",
      "|              6|  Active|   2020-01-10|   Alberta|    Brooks|Female|a.brooks@newmail.com| 896-2884-61| 97078|            null|2020-01-01|2020-01-10|  1|\n",
      "|              7|  Active|   2020-01-10|    Cherry|  Harrison|Female|c.harrison@newmai...| 069-7921-78|156759|            null|2020-01-01|2020-01-10|  1|\n",
      "|              8|  Active|   2020-01-10|     Edith|    Fowler|Female|e.fowler@newmail.com| 507-8262-99|132468|            null|2020-01-01|2020-01-10|  1|\n",
      "|              9|  Active|   2020-01-10|    Alfred|     Mason|  Male| a.mason@newmail.com| 718-7756-65| 48496|            null|2020-01-01|2020-01-10|  1|\n",
      "|             10|  Active|   2020-01-10|   Jessica|  Hamilton|Female|j.hamilton@newmai...| 670-5046-24|175777|            null|2020-01-01|2020-01-10|  1|\n",
      "|             11|  Active|   2020-01-10|  Lilianna|  Crawford|Female|l.crawford@newmai...| 339-0865-57|148448|            null|2020-01-01|2020-01-10|  1|\n",
      "|             12|  Active|   2020-01-10|      Jack|    Brooks|  Male|j.brooks@newmail.com| 020-1068-47| 77024|            null|2020-01-01|2020-01-10|  1|\n",
      "|             13|  Active|   2020-01-10|    Wilson|  Morrison|  Male|w.morrison@newmai...| 152-0725-67| 38296|            null|2020-01-01|2020-01-10|  1|\n",
      "|             14|  Active|   2020-01-10|     Cadie|     Evans|Female| c.evans@newmail.com| 360-2903-44|103136|            null|2020-01-01|2020-01-10|  1|\n",
      "|             15|  Active|   2020-01-10|  Adelaide|     Owens|Female| a.owens@newmail.com| 082-2307-17|130388|            null|2020-01-01|2020-01-10|  1|\n",
      "|             16|  Active|   2020-01-10|      Dale|    Morgan|  Male|d.morgan@newmail.com| 056-1660-19|161900|            null|2020-01-01|2020-01-10|  1|\n",
      "|             17|  Active|   2020-01-10|     Tiana|   Cameron|Female|t.cameron@newmail...| 941-8465-88|134040|            null|2020-01-01|2020-01-10|  1|\n",
      "|             18|  Active|   2020-01-10|      Eddy|  Ferguson|  Male|e.ferguson@newmai...| 351-1295-46|147343|            null|2020-01-01|2020-01-10|  1|\n",
      "|             19|  Active|   2020-01-10|    Sawyer|     Owens|  Male| s.owens@newmail.com| 216-0400-38| 70190|            null|2020-01-01|2020-01-10|  1|\n",
      "|             20|  Active|   2020-01-10|     Lucas|     Jones|  Male| l.jones@newmail.com| 146-8629-51| 80877|            null|2020-01-01|2020-01-10|  1|\n",
      "|             21|  Active|   2020-01-10|     Oscar|     Smith|  Male| o.smith@newmail.com| 935-1911-34| 80407|            null|2020-01-01|2020-01-10|  1|\n",
      "|             22|  Active|   2020-01-10|      Tony|  Hamilton|  Male|t.hamilton@newmai...| 481-7914-72|125895|            null|2020-01-01|2020-01-10|  1|\n",
      "|             23|  Active|   2020-01-10|      Dale|     Smith|  Male| d.smith@newmail.com| 022-9025-15| 43457|            null|2020-01-01|2020-01-10|  1|\n",
      "|             24|  Active|   2020-01-10|     Julia|Cunningham|Female|j.cunningham@newm...| 068-3478-85| 31744|            null|2020-01-01|2020-01-10|  1|\n",
      "|             25|  Active|   2020-01-03|    Stella|      Hunt|Female|  s.hunt@newmail.com| 194-7397-62|122746|            null|2020-01-01|2020-01-03|  1|\n",
      "|             25|Inactive|   2020-01-04|    Stella|      Hunt|Female|  s.hunt@newmail.com| 194-7397-62|122746|      2020-01-04|2020-01-04|2020-01-04|  1|\n",
      "|             26|  Active|   2020-01-10|    Marcus|     Casey|  Male| m.casey@newmail.com| 869-0917-23| 44508|            null|2020-01-01|2020-01-10|  1|\n",
      "|             27|  Active|   2020-01-10|  Adrianna|Montgomery|Female|a.montgomery@newm...| 131-6228-32| 77391|            null|2020-01-01|2020-01-10|  1|\n",
      "|             28|  Active|   2020-01-10|     Elise|   Farrell|Female|e.farrell@newmail...| 485-8347-57|101616|            null|2020-01-01|2020-01-10|  1|\n",
      "|             29|  Active|   2020-01-10|    Gianna|     Wells|Female| g.wells@newmail.com| 025-9261-71|116267|            null|2020-01-01|2020-01-10|  1|\n",
      "+---------------+--------+-------------+----------+----------+------+--------------------+------------+------+----------------+----------+----------+---+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# print some records of df_final to check how it looks\n",
    "df_final.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e61cde7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 102:=================================================>   (185 + 5) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# print the shape of df_final\n",
    "print((df_final.count(), len(df_final.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a3e828de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# save the df_final as employee table\n",
    "df_final \\\n",
    "    .select(\"employee_number\", \"status\", \"snapshot_date\", \"first_name\", \"last_name\", \"gender\", \"email\",\"phone_number\", \"salary\", \"termination_date\", \"valid_from\", \"valid_to\") \\\n",
    "    .coalesce(1) \\\n",
    "    .write.csv(path='../data/processed/employee/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e087c5f",
   "metadata": {},
   "source": [
    "### 7.2.bis) Number rows ordered by `snapshot_date` descending using SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b00c46d",
   "metadata": {},
   "source": [
    "I'm including the following code chunks because I first tried the solution implemented in 7.1) using Spark SQL. But I got the following error: `cannot resolve '``rn``' given input columns`. \n",
    "\n",
    "It seems Spark SQL cannot resolve the alias `rn` in the `WHERE` clause. If I try to run `ROW_NUMBER() ...` as a sub-query in the `WHERE` clause I got another error stating this sub-query in not allowed here.\n",
    "\n",
    "For illustration purpose, I commented out the `WHERE` clause below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0c4d8c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intermediate = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    employee_number\n",
    "    , status\n",
    "    , snapshot_date\n",
    "    , ROW_NUMBER() OVER(PARTITION BY employee_number, status ORDER BY snapshot_date) AS rn\n",
    "FROM table_df_result\n",
    "-- WHERE rn = 1\n",
    "GROUP BY employee_number, status, snapshot_date\n",
    "ORDER BY employee_number, status, snapshot_date\n",
    ";\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "97b90161",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 119:==========================================>          (160 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+-------------+---+\n",
      "|employee_number|  status|snapshot_date| rn|\n",
      "+---------------+--------+-------------+---+\n",
      "|              1|  Active|   2020-01-01|  1|\n",
      "|              2|  Active|   2020-01-01|  1|\n",
      "|              3|  Active|   2020-01-01|  1|\n",
      "|              4|  Active|   2020-01-01|  1|\n",
      "|              5|  Active|   2020-01-01|  1|\n",
      "|              6|  Active|   2020-01-01|  1|\n",
      "|              7|  Active|   2020-01-01|  1|\n",
      "|              8|  Active|   2020-01-01|  1|\n",
      "|              9|  Active|   2020-01-01|  1|\n",
      "|             10|  Active|   2020-01-01|  1|\n",
      "|             11|  Active|   2020-01-01|  1|\n",
      "|             12|  Active|   2020-01-01|  1|\n",
      "|             13|  Active|   2020-01-01|  1|\n",
      "|             14|  Active|   2020-01-01|  1|\n",
      "|             15|  Active|   2020-01-01|  1|\n",
      "|             16|  Active|   2020-01-01|  1|\n",
      "|             17|  Active|   2020-01-01|  1|\n",
      "|             18|  Active|   2020-01-01|  1|\n",
      "|             19|  Active|   2020-01-01|  1|\n",
      "|             20|  Active|   2020-01-01|  1|\n",
      "|             21|  Active|   2020-01-01|  1|\n",
      "|             22|  Active|   2020-01-01|  1|\n",
      "|             23|  Active|   2020-01-01|  1|\n",
      "|             24|  Active|   2020-01-01|  1|\n",
      "|             25|  Active|   2020-01-01|  1|\n",
      "|             25|Inactive|   2020-01-04|  1|\n",
      "|             26|  Active|   2020-01-01|  1|\n",
      "|             27|  Active|   2020-01-01|  1|\n",
      "|             28|  Active|   2020-01-01|  1|\n",
      "|             29|  Active|   2020-01-01|  1|\n",
      "+---------------+--------+-------------+---+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_intermediate \\\n",
    "    .filter(\"rn == 1\") \\\n",
    "    .show(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
