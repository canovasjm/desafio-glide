{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccc9a936",
   "metadata": {},
   "source": [
    "# Glide Technical Exercise  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bc92e0",
   "metadata": {},
   "source": [
    "## 1) Import CSV files to pyspark data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4335f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required classes\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e5dee51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/canovasjm/spark/spark-3.0.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/03/03 00:44:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# set up configuration\n",
    "conf = SparkConf() \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .setAppName('glide') \\\n",
    "    .set(\"spark.jars\", \"/home/canovasjm/spark/spark-3.0.3-bin-hadoop3.2/jars/gcs-connector-hadoop3-latest.jar\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.json.keyfile\", \"/home/canovasjm/.google/credentials/google_credentials.json\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feac02e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0108219a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# read CSV files located in GCS bucket\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"gs://dtc_data_lake_deng-338919/glide/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "926255d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+------+----------+---------+------+--------------------+------------+------+----------------+\n",
      "|snapshot_date|employee_number|status|first_name|last_name|gender|               email|phone_number|salary|termination_date|\n",
      "+-------------+---------------+------+----------+---------+------+--------------------+------------+------+----------------+\n",
      "|   2020-01-05|              1|Active| Frederick|   Barnes|  Male|f.barnes@newmail.com| 094-8926-78| 38582|            null|\n",
      "|   2020-01-05|              2|Active|    Alford|    Grant|  Male| a.grant@newmail.com| 389-8947-85| 53126|            null|\n",
      "|   2020-01-05|              3|Active|    Sydney|  Farrell|Female|s.farrell@newmail...| 187-8343-84|151217|            null|\n",
      "|   2020-01-05|              3|Active|    Sydney|  Farrell|Female|s.farrell@newmail...| 187-8343-84|151217|            null|\n",
      "|   2020-01-05|              4|Active|     Rosie| Richards|Female|r.richards@newmai...| 357-9337-53|162461|            null|\n",
      "|   2020-01-05|              5|Active|     Jared|   Wright|  Male|j.wright@newmail.com| 256-0454-59| 98258|            null|\n",
      "|   2020-01-05|              6|Active|   Alberta|   Brooks|Female|a.brooks@newmail.com| 896-2884-61| 97078|            null|\n",
      "|   2020-01-05|              7|Active|    Cherry| Harrison|Female|c.harrison@newmai...| 069-7921-78|156759|            null|\n",
      "|   2020-01-05|              8|Active|     Edith|   Fowler|Female|e.fowler@newmail.com| 507-8262-99|132468|            null|\n",
      "|   2020-01-05|              9|Active|    Alfred|    Mason|  Male| a.mason@newmail.com| 718-7756-65| 48496|            null|\n",
      "+-------------+---------------+------+----------+---------+------+--------------------+------------+------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# print first 10 rows to check if df looks fine\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bba943fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(495, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# print the shape of df\n",
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17a79aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(snapshot_date,StringType,true),StructField(employee_number,StringType,true),StructField(status,StringType,true),StructField(first_name,StringType,true),StructField(last_name,StringType,true),StructField(gender,StringType,true),StructField(email,StringType,true),StructField(phone_number,StringType,true),StructField(salary,StringType,true),StructField(termination_date,StringType,true)))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the df schema. As we see, all columns are read as string\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac54c91",
   "metadata": {},
   "source": [
    "So far our all columns in our data frame are of type string. This is not optimal and won't allow us to do further manipulations of this data frame, so we need to fix it before moving on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9f1c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ee0e1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a proper schema\n",
    "schema = types.StructType([\n",
    "    types.StructField('snapshot_date', types.DateType(), True),\n",
    "    types.StructField('employee_number', types.IntegerType(), True),\n",
    "    types.StructField('status', types.StringType(), True),\n",
    "    types.StructField('first_name', types.StringType(), True),\n",
    "    types.StructField('last_name', types.StringType(), True),\n",
    "    types.StructField('gender', types.StringType(), True),\n",
    "    types.StructField('email', types.StringType(), True),\n",
    "    types.StructField('phone_number', types.StringType(), True),\n",
    "    types.StructField('salary', types.IntegerType(), True),\n",
    "    types.StructField('termination_date', types.DateType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eac56916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read CSV files located in GCS bucket again, this time specifying the schema we defined above\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"gs://dtc_data_lake_deng-338919/glide/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce9aa686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(snapshot_date,DateType,true),StructField(employee_number,IntegerType,true),StructField(status,StringType,true),StructField(first_name,StringType,true),StructField(last_name,StringType,true),StructField(gender,StringType,true),StructField(email,StringType,true),StructField(phone_number,StringType,true),StructField(salary,IntegerType,true),StructField(termination_date,DateType,true)))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the schema\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61888b34",
   "metadata": {},
   "source": [
    "## 2)  Some data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7d2dde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+--------+----------+---------+------+------------------+------------+------+----------------+\n",
      "|snapshot_date|employee_number|  status|first_name|last_name|gender|             email|phone_number|salary|termination_date|\n",
      "+-------------+---------------+--------+----------+---------+------+------------------+------------+------+----------------+\n",
      "|   2020-01-04|             25|Inactive|    Stella|     Hunt|Female|s.hunt@newmail.com| 194-7397-62|122746|      2020-01-04|\n",
      "+-------------+---------------+--------+----------+---------+------+------------------+------------+------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.filter(\"termination_date IS NOT NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c7107a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(\"employee_number IS NULL\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b78cdd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2484dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5658996a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151021c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79914874",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
